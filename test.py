import requests
from datetime import datetime, timedelta
import urllib.request, urllib.parse, json, sys, re
from collections import defaultdict
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
import warnings

warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 0) KMA ë™ë„¤ì˜ˆë³´ ì§€ì—­ ëª©ë¡ ë¡œë”© (ìÂ·ë©´Â·ë™ ë‹¨ìœ„)
#    HTTPS/HTTP ëª¨ë‘ ì‹œë„, User-Agent ì¶”ê°€, ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë°œìƒ
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def load_kma_regions():
    urls = [
        "https://www.weather.go.kr/DFSROOT/POINT/DATA/top.json.txt",
        "http://www.weather.go.kr/DFSROOT/POINT/DATA/top.json.txt"
    ]
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    tree = None

    for url in urls:
        try:
            resp = requests.get(url, headers=headers, timeout=5)
            resp.raise_for_status()
            tree = resp.json()  # JSON íŒŒì‹± ì‹œë„
            print(f"âœ… {url} ë¡œë”© ì„±ê³µ")
            break
        except Exception as e:
            print(f"âš ï¸ {url} ë¡œë”© ì‹¤íŒ¨: {e}")

    if tree is None:
        raise RuntimeError("KMA ì§€ì—­ ëª©ë¡ì„ ëª¨ë‘ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ë‚˜ URLì„ í™•ì¸í•˜ì„¸ìš”.")

    regions = []
    def recurse(node):
        name = node.get("local")
        if name:
            regions.append(name)
        for child in node.get("children", []):
            recurse(child)
    recurse(tree)

    print(f"â–¶ ë¡œë“œëœ ì§€ì—­ ê°œìˆ˜: {len(regions)}")
    print(f"â–¶ 'ì—¬ìˆ˜' í¬í•¨ ì—¬ë¶€: {'ì—¬ìˆ˜' in regions}")
    return regions

# ì‹¤ì œ ë¡œë”©
REGIONS = load_kma_regions()

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 1) Visual Crossing ë‚ ì”¨ íƒ€ì„ë¼ì¸ API (ìµœëŒ€ 30ì¼)
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
VC_API_KEY = "2L3JELGRGWJHR4C8Q2BDDFN3U"

def fetch_timeline(region, start, end):
    loc = urllib.parse.quote(f"{region},KR")
    url = (
        f"https://weather.visualcrossing.com/VisualCrossingWebServices/"
        f"rest/services/timeline/{loc}/{start}/{end}"
        f"?unitGroup=metric&include=days&key={VC_API_KEY}&contentType=json"
    )
    r = requests.get(url)
    r.raise_for_status()
    return r.json().get("days", [])

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 2) OpenWeatherMap ë¯¸ì„¸ë¨¼ì§€ ì˜ˆë³´
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
OWM_API_KEY = "YOUR_OPENWEATHERMAP_KEY"
REGION_COORD = {
    'ì„œìš¸': (37.5665,126.9780), 'ë¶€ì‚°': (35.1796,129.0756),
    'ì¸ì²œ': (37.4563,126.7052), 'ëŒ€êµ¬': (35.8714,128.6014),
    'ê´‘ì£¼': (35.1595,126.8526), 'ìš¸ì‚°': (35.5384,129.3114),
    'ì œì£¼': (33.4996,126.5312),
}

def fetch_air_pollution(lat, lon):
    data = requests.get(
        "http://api.openweathermap.org/data/2.5/air_pollution/forecast",
        params={"lat":lat,"lon":lon,"appid":OWM_API_KEY}
    ).json().get("list", [])
    daily = defaultdict(lambda: {"pm2_5": [], "pm10": []})
    for e in data:
        dt = datetime.utcfromtimestamp(e["dt"]) + timedelta(hours=9)
        d = dt.date().isoformat()
        c = e["components"]
        daily[d]["pm2_5"].append(c.get("pm2_5", 0))
        daily[d]["pm10"].append(c.get("pm10", 0))
    return {d: {"pm2_5": sum(v["pm2_5"])/len(v["pm2_5"]),
                "pm10":  sum(v["pm10"])/len(v["pm10"])}
            for d, v in daily.items()}

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 3) ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë§›ì§‘ ê²€ìƒ‰
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
NAVER_CLIENT_ID     = "jh4j9OLJJV71zlU6TiBn"
NAVER_CLIENT_SECRET = "3uNogYwcqj"

def clean_html(raw):
    return re.sub(r'<.*?>', '', raw)

def search_blog(query):
    enc = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/blog?query={enc}&display=5"
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
    req.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
    with urllib.request.urlopen(req) as res:
        if res.getcode() != 200:
            return []
        data = json.loads(res.read().decode())
    return [{
        "title": clean_html(item["title"]),
        "link":  item["link"],
        "blogger": item["bloggername"],
        "date":   item["postdate"],
        "desc":   clean_html(item["description"])
    } for item in data.get("items",[])]

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ë©”ë‰´ êµ¬í˜„
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def menu_precip_regions():
    today = datetime.now().date()
    start, end = today.isoformat(), (today + timedelta(days=29)).isoformat()
    max_date = today + timedelta(days=29)
    print(f"ğŸ‘‰ ìµœëŒ€ ì˜ˆë³´ì¼ì: {max_date.month}.{max_date.day} (ì´ 30ì¼ì¹˜)")

    dr = input("\n[1-1] ë‚ ì§œ ì…ë ¥ (ì˜ˆ: 6.10-6.24 or 6.15) â–¶ ").strip()
    try:
        def to_off(p):
            m, d = p.split('.')
            return (datetime(today.year, int(m), int(d)).date() - today).days
        if '-' in dr:
            a, b = dr.split('-')
            offs = list(range(to_off(a), to_off(b) + 1))
        else:
            offs = [to_off(dr)]
    except:
        print("í˜•ì‹ ì˜¤ë¥˜ â€“ ì˜ˆì‹œ(6.10-6.24 or 6.15)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.")
        return

    if any(o < 0 or o > 29 for o in offs):
        print("\nì˜ˆë³´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤!")
        return

    stats = []
    for region in REGIONS:
        try:
            days = fetch_timeline(region, start, end)
            cnt = sum(1 for o in offs if days[o]["precipprob"] < 10)
            stats.append((region, cnt))
        except:
            continue

    if not stats:
        print("ì¡°íšŒ ê°€ëŠ¥í•œ ì§€ì—­ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    mx = max(c for _, c in stats)
    if mx == 0:
        print("\ní•´ë‹¹ ê¸°ê°„ì—ëŠ” ë§‘ì€ ì§€ì—­ì´ ì—†ìŠµë‹ˆë‹¤!")
        return

    print("\nâ–¶ ë§‘ì€ ë‚  ìµœë‹¤ ì§€ì—­ Top3:")
    for r, c in sorted(stats, key=lambda x: -x[1])[:3]:
        print(f" â€¢ {r} ({c}ì¼)")

def menu_precip_dates():
    today = datetime.now().date()
    start, end = today.isoformat(), (today + timedelta(days=29)).isoformat()

    user = input("\n[1-2] ì§€ì—­ ì…ë ¥ â–¶ ").strip()
    candidates = [r for r in REGIONS if user in r]
    if not candidates:
        print("ë“±ë¡ë˜ì§€ ì•Šì€ ì§€ì—­ì…ë‹ˆë‹¤!")
        return
    region = candidates[0]

    days = fetch_timeline(region, start, end)
    seqs, cur = [], []
    for i, day in enumerate(days):
        if day["precipprob"] < 10:
            cur.append(i)
        else:
            if cur:
                seqs.append(cur)
                cur = []
    if cur:
        seqs.append(cur)

    if not seqs:
        print("\ní•´ë‹¹ ì§€ì—­ì—ëŠ” í–¥í›„ ë§‘ì€ ë‚ ì´ ì—†ìŠµë‹ˆë‹¤!")
        return

    print(f"\nâ–¶ {region} ì—°ì† ë§‘ì€ ë‚ :")
    for seq in seqs:
        ds = today + timedelta(days=seq[0])
        de = today + timedelta(days=seq[-1])
        if len(seq) == 1:
            print(f" â€¢ {ds} (ë‹¨ì¼)")
        else:
            print(f" â€¢ {ds} ~ {de} ({len(seq)}ì¼)")

def menu_dust_dates():
    today = datetime.now().date()
    dr = input("\n[2-1] ë‚ ì§œ ì…ë ¥ (ì˜ˆ: 6.10 or 6.10-6.12) â–¶ ").strip()
    try:
        if '-' in dr:
            a, b = dr.split('-')
            def to_iso(p):
                m, d = p.split('.')
                return datetime(today.year, int(m), int(d)).date().isoformat()
            dates = [to_iso(a), to_iso(b)]
        else:
            m, d = dr.split('.')
            dates = [datetime(today.year, int(m), int(d)).date().isoformat()]
    except:
        print("í˜•ì‹ ì˜¤ë¥˜ â€“ ì˜ˆì‹œ(6.10 or 6.10-6.12)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.")
        return

    scores = {}
    for reg, coord in REGION_COORD.items():
        ap = fetch_air_pollution(*coord)
        vals = [ap[d]["pm2_5"] for d in dates if d in ap]
        if vals:
            scores[reg] = sum(vals) / len(vals)
    if not scores:
        print("\ní•´ë‹¹ ë‚ ì§œ ì˜ˆë³´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return

    print("\nâ–¶ ì§€ì • ë‚ ì§œ PM2.5 í‰ê·  ìµœì € ì§€ì—­ Top3:")
    for i, (r, avg) in enumerate(sorted(scores.items(), key=lambda x: x[1])[:3], 1):
        print(f" {i}. {r}: í‰ê·  PM2.5 {avg:.1f}Âµg/mÂ³")

def menu_dust_regions():
    user = input("\n[2-2] ì§€ì—­ ì…ë ¥ â–¶ ").strip()
    candidates = [r for r in REGION_COORD if user in r]
    if not candidates:
        print("ë“±ë¡ë˜ì§€ ì•Šì€ ì§€ì—­ì…ë‹ˆë‹¤!")
        return
    region = candidates[0]
    ap = fetch_air_pollution(*REGION_COORD[region])
    print(f"\nâ–¶ {region} PM2.5 ìµœì € Top3:")
    for date, v in sorted(ap.items(), key=lambda x: x[1]["pm2_5"])[:3]:
        print(f" â€¢ {date}: PM2.5 {v['pm2_5']:.1f}Âµg/mÂ³, PM10 {v['pm10']:.1f}Âµg/mÂ³")

def menu_sun():
    today = datetime.now().date()
    user = input("\n[3] ì§€ì—­ ì…ë ¥ â–¶ ").strip()
    candidates = [r for r in REGIONS if user in r]
    if not candidates:
        print("ë“±ë¡ë˜ì§€ ì•Šì€ ì§€ì—­ì…ë‹ˆë‹¤!")
        return
    region = candidates[0]
    days = fetch_timeline(region, today.isoformat(), (today + timedelta(days=1)).isoformat())
    print(f"\nâ–¶ {region} ì¼ì¶œÂ·ì¼ëª°:")
    for i, label in enumerate(["ì˜¤ëŠ˜", "ë‚´ì¼"]):
        print(f" {label}({days[i]['datetime']}): ì¼ì¶œ {days[i]['sunrise']}, ì¼ëª° {days[i]['sunset']}")

def menu_restaurant():
    q = input("\n[4] ê²€ìƒ‰ì–´ ì…ë ¥ â–¶ ").strip()
    if not q:
        print("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”!")
        return
    res = search_blog(q)
    if not res:
        print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    print(f"\nâ–¶ '{q}' ë§›ì§‘ ê²€ìƒ‰ ê²°ê³¼:")
    for i, it in enumerate(res, 1):
        print(f"{i}. {it['title']}\n   ë§í¬: {it['link']}\n   ë¸”ë¡œê±°: {it['blogger']} ({it['date']})\n   ìš”ì•½: {it['desc']}\n")

def main():
    while True:
        print("\n" + "="*40)
        print("    âœˆï¸  ì—¬í–‰ ë‚ ì”¨ & ì •ë³´ ë©”ë‰´  âœˆï¸")
        print("="*40)
        print(" 1. ê°•ìˆ˜ëŸ‰ìœ¼ë¡œ ì—¬í–‰ ì¶”ì²œ")
        print(" 2. ë¯¸ì„¸ë¨¼ì§€ë¡œ ì—¬í–‰ ì¶”ì²œ")
        print(" 3. ì—¬í–‰ì§€ ì¼ì¶œì¼ëª°ì‹œê°„ ê²€ìƒ‰")
        print(" 4. ì—¬í–‰ì§€ ë§›ì§‘ ê²€ìƒ‰")
        print(" 0. ì¢…ë£Œ")
        print("="*40)
        c = input("ë©”ë‰´ â–¶ ").strip()
        if c == '1':
            print("\n 1) ë‚ ì§œë¡œ ì§€ì—­ ì¶”ì²œ   2) ì§€ì—­ìœ¼ë¡œ ë‚ ì§œ ì¶”ì²œ")
            s = input("â–¶ ").strip()
            if s == '1':
                menu_precip_regions()
            elif s == '2':
                menu_precip_dates()
            else:
                print("â†’ 1 ë˜ëŠ” 2ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        elif c == '2':
            print("\n 1) ë‚ ì§œë¡œ ì§€ì—­ ì¶”ì²œ   2) ì§€ì—­ìœ¼ë¡œ ë‚ ì§œ ì¶”ì²œ")
            s = input("â–¶ ").strip()
            if s == '1':
                menu_dust_dates()
            elif s == '2':
               	menu_dust_regions()
            else:
                print("â†’ 1 ë˜ëŠ” 2ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        elif c == '3':
            menu_sun()
        elif c == '4':
            menu_restaurant()
        elif c == '0':
            print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
            break
        else:
            print("â†’ 1~4 ë˜ëŠ” 0ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
